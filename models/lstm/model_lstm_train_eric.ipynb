{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mNzUyFnutJeV",
        "outputId": "4a226b65-ca94-4c83-fa7b-e56dae35c3c0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nExample script to train a network to generate text with the style of a given corpus\\n--By word--\\nIt is recommended to run this script on GPU, as recurrent\\nnetworks are quite computationally intensive.\\nBased on\\nhttps://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\\n20 epochs should be enough to get decent results.\\nUses data generator to avoid loading all the test set into memory.\\nSaves the weights and model every epoch.\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Example script to train a network to generate text with the style of a given corpus\n",
        "--By word--\n",
        "It is recommended to run this script on GPU, as recurrent\n",
        "networks are quite computationally intensive.\n",
        "Based on\n",
        "https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "20 epochs should be enough to get decent results.\n",
        "Uses data generator to avoid loading all the test set into memory.\n",
        "Saves the weights and model every epoch.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jihZr_bPtKss"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional\n",
        "import numpy as np\n",
        "import sys\n",
        "import io\n",
        "import os\n",
        "import codecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc1rPDN4tKku"
      },
      "outputs": [],
      "source": [
        "# Parameters: change to experiment different configurations\n",
        "SEQUENCE_LEN = 10\n",
        "MIN_WORD_FREQUENCY = 10\n",
        "STEP = 1\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDwOiNM8tKRF"
      },
      "outputs": [],
      "source": [
        "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
        "    # shuffle at unison\n",
        "    print('Shuffling sentences')\n",
        "\n",
        "    tmp_sentences = []\n",
        "    tmp_next_word = []\n",
        "    for i in np.random.permutation(len(sentences_original)):\n",
        "        tmp_sentences.append(sentences_original[i])\n",
        "        tmp_next_word.append(next_original[i])\n",
        "\n",
        "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
        "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
        "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
        "\n",
        "    print(\"Size of training set = %d\" % len(x_train))\n",
        "    print(\"Size of test set = %d\" % len(y_test))\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "\n",
        "# Data generator for fit and evaluate\n",
        "def generator(sentence_list, next_word_list, batch_size):\n",
        "    index = 0\n",
        "    while True:\n",
        "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=bool)\n",
        "        y = np.zeros((batch_size, len(words)), dtype=bool)\n",
        "        for i in range(batch_size):\n",
        "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
        "                x[i, t, word_indices[w]] = 1\n",
        "            y[i, word_indices[next_word_list[index % len(sentence_list)]]] = 1\n",
        "            index = index + 1\n",
        "        yield x, y\n",
        "\n",
        "\n",
        "def print_vocabulary(words_file_path, words_set):\n",
        "    words_file = codecs.open(words_file_path, 'w', encoding='utf8')\n",
        "    for w in words_set:\n",
        "        if w != \"\\n\":\n",
        "            words_file.write(w+\"\\n\")\n",
        "        else:\n",
        "            words_file.write(w)\n",
        "    words_file.close()\n",
        "\n",
        "\n",
        "def get_model(dropout=0.2):\n",
        "    print('Build model...')\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(128), input_shape=(SEQUENCE_LEN, len(words))))\n",
        "    if dropout > 0:\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(len(words)))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "\n",
        "\n",
        "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
        "\n",
        "    # Randomly pick a seed sequence\n",
        "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
        "    seed = (sentences+sentences_test)[seed_index]\n",
        "\n",
        "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
        "        sentence = seed\n",
        "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
        "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
        "        examples_file.write(' '.join(sentence))\n",
        "\n",
        "        for i in range(50):\n",
        "            x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
        "            for t, word in enumerate(sentence):\n",
        "                x_pred[0, t, word_indices[word]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_word = indices_word[next_index]\n",
        "\n",
        "            sentence = sentence[1:]\n",
        "            sentence.append(next_word)\n",
        "\n",
        "            examples_file.write(\" \"+next_word)\n",
        "        examples_file.write('\\n')\n",
        "    examples_file.write('='*80 + '\\n')\n",
        "    examples_file.flush()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMJAwpChs12r",
        "outputId": "e7aeda3f-c1be-4f5b-b4c0-7d068d224eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus length in characters: 72959776\n",
            "Corpus length in words: 15801923\n",
            "Unique words before ignoring: 248649\n",
            "Ignoring words with frequency < 10\n",
            "Unique words after ignoring: 28939\n",
            "Ignored sequences: 3196843\n",
            "Remaining sequences: 12605070\n"
          ]
        }
      ],
      "source": [
        "with open('../content/drive/MyDrive/Data/select_lyrics_data.txt', encoding='utf-8') as f:\n",
        "    text = f.read().lower().replace('\\n', ' \\n ')\n",
        "print('Corpus length in characters:', len(text))\n",
        "\n",
        "text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
        "print('Corpus length in words:', len(text_in_words))\n",
        "\n",
        "# Calculate word frequency\n",
        "word_freq = {}\n",
        "for word in text_in_words:\n",
        "    word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "ignored_words = set()\n",
        "for k, v in word_freq.items():\n",
        "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
        "        ignored_words.add(k)\n",
        "\n",
        "words = set(text_in_words)\n",
        "print('Unique words before ignoring:', len(words))\n",
        "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
        "words = sorted(set(words) - ignored_words)\n",
        "print('Unique words after ignoring:', len(words))\n",
        "print_vocabulary('../content/drive/MyDrive/Data/vocab_v4.txt', words)\n",
        "\n",
        "word_indices = dict((c, i) for i, c in enumerate(words))\n",
        "indices_word = dict((i, c) for i, c in enumerate(words))\n",
        "\n",
        "# cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
        "sentences = []\n",
        "next_words = []\n",
        "ignored = 0\n",
        "for i in range(0, len(text_in_words) - SEQUENCE_LEN, STEP):\n",
        "    # Only add the sequences where no word is in ignored_words\n",
        "    if len(set(text_in_words[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
        "        sentences.append(text_in_words[i: i + SEQUENCE_LEN])\n",
        "        next_words.append(text_in_words[i + SEQUENCE_LEN])\n",
        "    else:\n",
        "        ignored = ignored + 1\n",
        "print('Ignored sequences:', ignored)\n",
        "print('Remaining sequences:', len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI7-zci1vA8O",
        "outputId": "e48bba6c-7407-4d54-c5e5-86c159dae431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shuffling sentences\n",
            "Size of training set = 490000\n",
            "Size of test set = 10000\n"
          ]
        }
      ],
      "source": [
        "# x, y, x_test, y_test\n",
        "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(\n",
        "    sentences[:500000], next_words[:500000]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVUxBwdeu2Ze",
        "outputId": "35518f4f-a2ef-47a2-d943-5801b75eb45a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Epoch 1/25\n",
            "15313/15313 [==============================] - ETA: 0s - loss: 5.2775 - accuracy: 0.2066"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 607s 39ms/step - loss: 5.2775 - accuracy: 0.2066 - val_loss: 4.6718 - val_accuracy: 0.2541\n",
            "Epoch 2/25\n",
            "15313/15313 [==============================] - ETA: 0s - loss: 4.4219 - accuracy: 0.2681"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 597s 39ms/step - loss: 4.4219 - accuracy: 0.2681 - val_loss: 4.3367 - val_accuracy: 0.2861\n",
            "Epoch 3/25\n",
            "15313/15313 [==============================] - ETA: 0s - loss: 3.9517 - accuracy: 0.3158"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 597s 39ms/step - loss: 3.9517 - accuracy: 0.3158 - val_loss: 4.1607 - val_accuracy: 0.3122\n",
            "Epoch 4/25\n",
            "15313/15313 [==============================] - ETA: 0s - loss: 3.5599 - accuracy: 0.3640"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 595s 39ms/step - loss: 3.5599 - accuracy: 0.3640 - val_loss: 4.1027 - val_accuracy: 0.3302\n",
            "Epoch 5/25\n",
            "15312/15313 [============================>.] - ETA: 0s - loss: 3.2122 - accuracy: 0.4101"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 597s 39ms/step - loss: 3.2122 - accuracy: 0.4101 - val_loss: 4.1196 - val_accuracy: 0.3423\n",
            "Epoch 6/25\n",
            "15312/15313 [============================>.] - ETA: 0s - loss: 2.9060 - accuracy: 0.4527"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 597s 39ms/step - loss: 2.9060 - accuracy: 0.4527 - val_loss: 4.1621 - val_accuracy: 0.3493\n",
            "Epoch 7/25\n",
            "15312/15313 [============================>.] - ETA: 0s - loss: 2.6388 - accuracy: 0.4921"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 596s 39ms/step - loss: 2.6388 - accuracy: 0.4921 - val_loss: 4.2070 - val_accuracy: 0.3594\n",
            "Epoch 8/25\n",
            "15313/15313 [==============================] - ETA: 0s - loss: 2.4104 - accuracy: 0.5275"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 595s 39ms/step - loss: 2.4104 - accuracy: 0.5275 - val_loss: 4.2506 - val_accuracy: 0.3600\n",
            "Epoch 9/25\n",
            "15313/15313 [==============================] - ETA: 0s - loss: 2.2087 - accuracy: 0.5591"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 595s 39ms/step - loss: 2.2087 - accuracy: 0.5591 - val_loss: 4.3303 - val_accuracy: 0.3641\n",
            "Epoch 10/25\n",
            "15312/15313 [============================>.] - ETA: 0s - loss: 2.0398 - accuracy: 0.5870"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 597s 39ms/step - loss: 2.0398 - accuracy: 0.5870 - val_loss: 4.3851 - val_accuracy: 0.3662\n",
            "Epoch 11/25\n",
            "15313/15313 [==============================] - ETA: 0s - loss: 1.8885 - accuracy: 0.6118"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15313/15313 [==============================] - 598s 39ms/step - loss: 1.8885 - accuracy: 0.6118 - val_loss: 4.4237 - val_accuracy: 0.3701\n",
            "Epoch 12/25\n",
            "15100/15313 [============================>.] - ETA: 8s - loss: 1.7527 - accuracy: 0.6350"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "file_path = \"../content/drive/MyDrive/Data/Logs/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-\" \\\n",
        "            \"loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % \\\n",
        "            (len(words), SEQUENCE_LEN, MIN_WORD_FREQUENCY)\n",
        "\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
        "callbacks_list = [print_callback, early_stopping]\n",
        "\n",
        "examples_file = open('../content/drive/MyDrive/Data/Logs/Example.txt', \"w\")\n",
        "model.fit(  generator(sentences, next_words, BATCH_SIZE),\n",
        "            steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
        "            epochs=25,\n",
        "            callbacks=callbacks_list,\n",
        "            validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
        "            validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA4wUtZHd_zt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WrEtgv79BgL"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxA20uZipE5S"
      },
      "outputs": [],
      "source": [
        "model.save('../content/drive/MyDrive/Data/LSTM_simple_model_v4.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 ('.env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "a56046da640a8b62189484cf1ab9a225355c89bf0946515608a942c5510f10bc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
